{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tokenomics Blockchain Decentralization - Documentation This is the documentation for the Tokenomics Decentralization Analysis tool developed by the University of Edinburgh's Blockchain Technology Lab. The tool is responsible for analyzing the token distribution of various blockchains and measuring their subsequent levels of decentralization. The relevant source code is available on GitHub . Overview Currently, the supported ledgers are: Bitcoin Bitcoin Cash Cardano Dogecoin Ethereum Litecoin Tezos We intend to add more ledgers to this list in the future. Contributing This is an open source project licensed under the terms and conditions of the MIT license and CC BY-SA 4.0 . Everyone is welcome to contribute to it by proposing or implementing their ideas. Example contributions include, but are not limited to, reporting potential bugs, supplying useful information for the clustering of supported ledgers, adding support for a new ledger, or making the code more efficient. All contributions to the project will also be covered by the above-mentioned license. When making changes in the code, contributors are required to fork the project's repository first and then issue a pull request with their changes. Each PR will be reviewed before being merged to the main branch. Bugs can be reported in the Issues page. Other comments and ideas can be brought up in the project's Discussions . For more information on how to make specific contributions, see How to Contribute .","title":"Home"},{"location":"#tokenomics-blockchain-decentralization-documentation","text":"This is the documentation for the Tokenomics Decentralization Analysis tool developed by the University of Edinburgh's Blockchain Technology Lab. The tool is responsible for analyzing the token distribution of various blockchains and measuring their subsequent levels of decentralization. The relevant source code is available on GitHub .","title":"Tokenomics Blockchain Decentralization - Documentation"},{"location":"#overview","text":"Currently, the supported ledgers are: Bitcoin Bitcoin Cash Cardano Dogecoin Ethereum Litecoin Tezos We intend to add more ledgers to this list in the future.","title":"Overview"},{"location":"#contributing","text":"This is an open source project licensed under the terms and conditions of the MIT license and CC BY-SA 4.0 . Everyone is welcome to contribute to it by proposing or implementing their ideas. Example contributions include, but are not limited to, reporting potential bugs, supplying useful information for the clustering of supported ledgers, adding support for a new ledger, or making the code more efficient. All contributions to the project will also be covered by the above-mentioned license. When making changes in the code, contributors are required to fork the project's repository first and then issue a pull request with their changes. Each PR will be reviewed before being merged to the main branch. Bugs can be reported in the Issues page. Other comments and ideas can be brought up in the project's Discussions . For more information on how to make specific contributions, see How to Contribute .","title":"Contributing"},{"location":"contribute/","text":"How to contribute You can contribute to the tool by adding support for a ledger, updating the mapping process for an existing ledger, or adding a new metric. In all cases, the information should be submitted via a GitHub PR. Add support for ledgers You can add support for a ledger that is not already supported as follows. Mapping information In the directory mapping_information/ , there exist two folders: addresses and special_addresses . addresses contains information about the owner or manager of an address. This information should be publicly available and verifiable, for example it may come from a public explorer, social media or forum posts, articles, etc. Each file in this folder is named <project_name>.jsonl (for the corresponding ledger) and contains a dictionary per line with the following information: (i) the address; (ii) the name of the entity (that controls the address); (iii) the source of the information (e.g., an explorer's URL); (iv) (optional) a boolean value is_contract (if omitted then it is assumed false); (v) (optional) extra_info that might be relevant or interesting (not used for the analysis). special_addresses contains information about addresses that should be treated specially, e.g., excluded from the analysis. This includes burn addresses, protocol-related addresses (e.g., Ethereum's staking contract), treasury addresses, etc. Here each file is named <project_name>.json and contains a list of dictionaries with the following information: (i) the address; (ii) the source of the information; (iii) extra_info which describes the reason why the address is special. To contribute mapping information you can either update an existing file, by changing and/or adding some entries, or create a new file for a newly-supported ledger. Price information The directory price_data/ contains information about the supported ledgers' market price. Each file in this folder is named <project_name>.csv (for the corresponding ledger). The csv file has no header and each line contains two comma-separated values: (i) a day (in the form YYYY-MM-DD); (ii) the USD market price of the token on the set day. To contribute price information you can either update an existing file, by adding entries for days where data is missing, or create a new file for a newly-supported ledger and add historical price data. Add metrics To add a new metric, you should do the following steps. First, create a relevant function in the script tokenomics_decentralization/metrics.py . The function should be named compute_{metric_name} and is given two parameters: (i) a list of tuples, where each tuple's first value is a numeric type that defines the balance of an address; (ii) an integer that defines the circulation (that is the sum of all address balances). Second, import this new function to tokenomics_decentralization/analyze.py . In this file, include the function as the value to the dictionary compute_functions of the analyze_snapshot function, using as a key the name of the function (which will be used in the config file). Third, add the name of the metric (which was used as the key to the dictionary in analyze.py ) to the file config.yaml under metrics . You can optionally also add it under the plot parameters, if you want it to be included in the plots by default. Finally, you should add unit tests for the new metric here and update the corresponding documentation page","title":"How to contribute"},{"location":"contribute/#how-to-contribute","text":"You can contribute to the tool by adding support for a ledger, updating the mapping process for an existing ledger, or adding a new metric. In all cases, the information should be submitted via a GitHub PR.","title":"How to contribute"},{"location":"contribute/#add-support-for-ledgers","text":"You can add support for a ledger that is not already supported as follows.","title":"Add support for ledgers"},{"location":"contribute/#mapping-information","text":"In the directory mapping_information/ , there exist two folders: addresses and special_addresses . addresses contains information about the owner or manager of an address. This information should be publicly available and verifiable, for example it may come from a public explorer, social media or forum posts, articles, etc. Each file in this folder is named <project_name>.jsonl (for the corresponding ledger) and contains a dictionary per line with the following information: (i) the address; (ii) the name of the entity (that controls the address); (iii) the source of the information (e.g., an explorer's URL); (iv) (optional) a boolean value is_contract (if omitted then it is assumed false); (v) (optional) extra_info that might be relevant or interesting (not used for the analysis). special_addresses contains information about addresses that should be treated specially, e.g., excluded from the analysis. This includes burn addresses, protocol-related addresses (e.g., Ethereum's staking contract), treasury addresses, etc. Here each file is named <project_name>.json and contains a list of dictionaries with the following information: (i) the address; (ii) the source of the information; (iii) extra_info which describes the reason why the address is special. To contribute mapping information you can either update an existing file, by changing and/or adding some entries, or create a new file for a newly-supported ledger.","title":"Mapping information"},{"location":"contribute/#price-information","text":"The directory price_data/ contains information about the supported ledgers' market price. Each file in this folder is named <project_name>.csv (for the corresponding ledger). The csv file has no header and each line contains two comma-separated values: (i) a day (in the form YYYY-MM-DD); (ii) the USD market price of the token on the set day. To contribute price information you can either update an existing file, by adding entries for days where data is missing, or create a new file for a newly-supported ledger and add historical price data.","title":"Price information"},{"location":"contribute/#add-metrics","text":"To add a new metric, you should do the following steps. First, create a relevant function in the script tokenomics_decentralization/metrics.py . The function should be named compute_{metric_name} and is given two parameters: (i) a list of tuples, where each tuple's first value is a numeric type that defines the balance of an address; (ii) an integer that defines the circulation (that is the sum of all address balances). Second, import this new function to tokenomics_decentralization/analyze.py . In this file, include the function as the value to the dictionary compute_functions of the analyze_snapshot function, using as a key the name of the function (which will be used in the config file). Third, add the name of the metric (which was used as the key to the dictionary in analyze.py ) to the file config.yaml under metrics . You can optionally also add it under the plot parameters, if you want it to be included in the plots by default. Finally, you should add unit tests for the new metric here and update the corresponding documentation page","title":"Add metrics"},{"location":"data/","text":"Data collection Currently, the data for the analysis of the different ledgers is collected through Google BigQuery . Queries One can retrieve the data directly from BigQuery using the queries below: Bitcoin WITH double_entry_book AS ( SELECT array_to_string(inputs.addresses, \",\") as address, inputs.type, -inputs.value as value FROM `bigquery-public-data.crypto_bitcoin.inputs` as inputs WHERE block_timestamp < \"{{timestamp}}\" UNION ALL SELECT array_to_string(outputs.addresses, \",\") as address, outputs.type, outputs.value as value FROM `bigquery-public-data.crypto_bitcoin.outputs` as outputs WHERE block_timestamp < \"{{timestamp}}\" ) SELECT address, type, sum(value) as balance FROM double_entry_book GROUP BY 1,2 HAVING balance > 0 ORDER BY balance DESC Bitcoin Cash WITH double_entry_book AS ( SELECT array_to_string(inputs.addresses, \",\") as address, inputs.type, -inputs.value as value FROM `bigquery-public-data.crypto_bitcoin_cash.inputs` as inputs WHERE block_timestamp < \"{{timestamp}}\" UNION ALL SELECT array_to_string(outputs.addresses, \",\") as address, outputs.type, outputs.value as value FROM `bigquery-public-data.crypto_bitcoin_cash.outputs` as outputs WHERE block_timestamp < \"{{timestamp}}\" ) SELECT address, type, sum(value) as balance FROM double_entry_book GROUP BY 1,2 HAVING balance > 0 ORDER BY balance DESC Dogecoin WITH double_entry_book AS ( SELECT array_to_string(inputs.addresses, \",\") as address, inputs.type, -inputs.value as value FROM `bigquery-public-data.crypto_dogecoin.inputs` as inputs WHERE block_timestamp < \"{{timestamp}}\" UNION ALL SELECT array_to_string(outputs.addresses, \",\") as address, outputs.type, outputs.value as value FROM `bigquery-public-data.crypto_dogecoin.outputs` as outputs WHERE block_timestamp < \"{{timestamp}}\" ) SELECT address, type, sum(value) as balance FROM double_entry_book GROUP BY 1,2 HAVING balance > 0 ORDER BY balance DESC Ethereum WITH double_entry_book AS ( SELECT to_address as address, value AS value FROM `bigquery-public-data.crypto_ethereum.traces` WHERE to_address IS NOT null AND status = 1 AND (call_type NOT IN ('delegatecall', 'callcode', 'staticcall') OR call_type IS null) AND block_timestamp < \"{{timestamp}}\" UNION ALL SELECT from_address as address, -value AS value FROM `bigquery-public-data.crypto_ethereum.traces` WHERE from_address IS NOT null AND status = 1 AND (call_type NOT IN ('delegatecall', 'callcode', 'staticcall') OR call_type IS null) AND block_timestamp < \"{{timestamp}}\" UNION ALL SELECT miner AS address, sum(cast(receipt_gas_used as numeric) * cast(gas_price as numeric)) AS value FROM `bigquery-public-data.crypto_ethereum.transactions` AS transactions JOIN `bigquery-public-data.crypto_ethereum.blocks` AS blocks on blocks.number = transactions.block_number WHERE transactions.block_timestamp < \"{{timestamp}}\" GROUP BY blocks.miner UNION ALL SELECT from_address AS address, -(cast(receipt_gas_used as numeric) * cast(gas_price as numeric)) AS value FROM `bigquery-public-data.crypto_ethereum.transactions` WHERE block_timestamp < \"{{timestamp}}\" ) SELECT address, sum(value) AS balance FROM double_entry_book GROUP BY address HAVING balance > 0 ORDER BY balance DESC Litecoin WITH double_entry_book AS ( SELECT array_to_string(inputs.addresses, \",\") as address, inputs.type, -inputs.value as value FROM `bigquery-public-data.crypto_litecoin.inputs` as inputs WHERE block_timestamp < \"{{timestamp}}\" UNION ALL SELECT array_to_string(outputs.addresses, \",\") as address, outputs.type, outputs.value as value FROM `bigquery-public-data.crypto_litecoin.outputs` as outputs WHERE block_timestamp < \"{{timestamp}}\" ) SELECT address, type, sum(value) as balance FROM double_entry_book GROUP BY 1,2 HAVING balance > 0 ORDER BY balance DESC Tezos WITH double_entry_book as ( SELECT IF(kind = 'contract', contract, delegate) AS address, change AS value FROM `public-data-finance.crypto_tezos.balance_updates` WHERE (status IS NULL OR status = 'applied') AND (timestamp < \"{{timestamp}}\") UNION ALL SELECT address, balance_change FROM `public-data-finance.crypto_tezos.migrations` WHERE timestamp < \"{{timestamp}}\" ) SELECT address, SUM(value) AS balance FROM double_entry_book GROUP BY address HAVING balance > 0 ORDER BY balance DESC Zcash WITH double_entry_book AS ( SELECT array_to_string(inputs.addresses, \",\") as address, inputs.type, -inputs.value as value FROM `bigquery-public-data.crypto_zcash.inputs` as inputs WHERE block_timestamp < \"{{timestamp}}\" UNION ALL SELECT array_to_string(outputs.addresses, \",\") as address, outputs.type, outputs.value as value FROM `bigquery-public-data.crypto_zcash.outputs` as outputs WHERE block_timestamp < \"{{timestamp}}\" ) SELECT address, type, sum(value) as balance FROM double_entry_book GROUP BY 1,2 HAVING balance > 0 ORDER BY balance DESC Automating the data collection process Instead of executing each of these queries separately on the BigQuery console and saving the results manually, it is also possible to automate the process using a script and collect all relevant data in one go. Executing this script will run queries from this file . IMPORTANT: the script uses service account credentials for authentication, therefore before running it, you need to generate the relevant credentials from Google, as described here and save your key in the data_collections_scripts/ directory of the project under the name 'google-service-account-key-0.json'. Any additional keys should be named 'google-service-account-key-1.json', 'google-service-account-key-2.json', and so on. There is a sample file that you can consult, which shows what your credentials are supposed to look like (but note that this is for informational purposes only, this file is not used in the code). Once you have set up the credentials, you can just run the following command from the root directory to retrieve data for all supported blockchains: python -m data_collection_scripts.big_query_balance_data There are also three command line arguments that can be used to customize the data collection process: ledgers accepts any number of the supported ledgers (case-insensitive). For example, adding --ledgers bitcoin results in collecting data only for Bitcoin, while --ledgers Bitcoin Ethereum would collect data for Bitcoin and Ethereum. If the ledgers argument is omitted, then the default value is used, which is taken from the configuration file and typically corresponds to all supported blockchains. snapshot_dates accepts any number of dates formatted as YYYY-MM-DD, YYYY-MM, or YYYY. Then, data is collected for the specified date(s). Again, if this argument is omitted, the default value is taken from the configuration file . --force-query forces the collection of all raw data files, even if some or all of the files already exist. By default, this flag is set to False and the script only fetches data for some blockchain if the corresponding file does not already exist.","title":"Data Collection"},{"location":"data/#data-collection","text":"Currently, the data for the analysis of the different ledgers is collected through Google BigQuery .","title":"Data collection"},{"location":"data/#queries","text":"One can retrieve the data directly from BigQuery using the queries below:","title":"Queries"},{"location":"data/#bitcoin","text":"WITH double_entry_book AS ( SELECT array_to_string(inputs.addresses, \",\") as address, inputs.type, -inputs.value as value FROM `bigquery-public-data.crypto_bitcoin.inputs` as inputs WHERE block_timestamp < \"{{timestamp}}\" UNION ALL SELECT array_to_string(outputs.addresses, \",\") as address, outputs.type, outputs.value as value FROM `bigquery-public-data.crypto_bitcoin.outputs` as outputs WHERE block_timestamp < \"{{timestamp}}\" ) SELECT address, type, sum(value) as balance FROM double_entry_book GROUP BY 1,2 HAVING balance > 0 ORDER BY balance DESC","title":"Bitcoin"},{"location":"data/#bitcoin-cash","text":"WITH double_entry_book AS ( SELECT array_to_string(inputs.addresses, \",\") as address, inputs.type, -inputs.value as value FROM `bigquery-public-data.crypto_bitcoin_cash.inputs` as inputs WHERE block_timestamp < \"{{timestamp}}\" UNION ALL SELECT array_to_string(outputs.addresses, \",\") as address, outputs.type, outputs.value as value FROM `bigquery-public-data.crypto_bitcoin_cash.outputs` as outputs WHERE block_timestamp < \"{{timestamp}}\" ) SELECT address, type, sum(value) as balance FROM double_entry_book GROUP BY 1,2 HAVING balance > 0 ORDER BY balance DESC","title":"Bitcoin Cash"},{"location":"data/#dogecoin","text":"WITH double_entry_book AS ( SELECT array_to_string(inputs.addresses, \",\") as address, inputs.type, -inputs.value as value FROM `bigquery-public-data.crypto_dogecoin.inputs` as inputs WHERE block_timestamp < \"{{timestamp}}\" UNION ALL SELECT array_to_string(outputs.addresses, \",\") as address, outputs.type, outputs.value as value FROM `bigquery-public-data.crypto_dogecoin.outputs` as outputs WHERE block_timestamp < \"{{timestamp}}\" ) SELECT address, type, sum(value) as balance FROM double_entry_book GROUP BY 1,2 HAVING balance > 0 ORDER BY balance DESC","title":"Dogecoin"},{"location":"data/#ethereum","text":"WITH double_entry_book AS ( SELECT to_address as address, value AS value FROM `bigquery-public-data.crypto_ethereum.traces` WHERE to_address IS NOT null AND status = 1 AND (call_type NOT IN ('delegatecall', 'callcode', 'staticcall') OR call_type IS null) AND block_timestamp < \"{{timestamp}}\" UNION ALL SELECT from_address as address, -value AS value FROM `bigquery-public-data.crypto_ethereum.traces` WHERE from_address IS NOT null AND status = 1 AND (call_type NOT IN ('delegatecall', 'callcode', 'staticcall') OR call_type IS null) AND block_timestamp < \"{{timestamp}}\" UNION ALL SELECT miner AS address, sum(cast(receipt_gas_used as numeric) * cast(gas_price as numeric)) AS value FROM `bigquery-public-data.crypto_ethereum.transactions` AS transactions JOIN `bigquery-public-data.crypto_ethereum.blocks` AS blocks on blocks.number = transactions.block_number WHERE transactions.block_timestamp < \"{{timestamp}}\" GROUP BY blocks.miner UNION ALL SELECT from_address AS address, -(cast(receipt_gas_used as numeric) * cast(gas_price as numeric)) AS value FROM `bigquery-public-data.crypto_ethereum.transactions` WHERE block_timestamp < \"{{timestamp}}\" ) SELECT address, sum(value) AS balance FROM double_entry_book GROUP BY address HAVING balance > 0 ORDER BY balance DESC","title":"Ethereum"},{"location":"data/#litecoin","text":"WITH double_entry_book AS ( SELECT array_to_string(inputs.addresses, \",\") as address, inputs.type, -inputs.value as value FROM `bigquery-public-data.crypto_litecoin.inputs` as inputs WHERE block_timestamp < \"{{timestamp}}\" UNION ALL SELECT array_to_string(outputs.addresses, \",\") as address, outputs.type, outputs.value as value FROM `bigquery-public-data.crypto_litecoin.outputs` as outputs WHERE block_timestamp < \"{{timestamp}}\" ) SELECT address, type, sum(value) as balance FROM double_entry_book GROUP BY 1,2 HAVING balance > 0 ORDER BY balance DESC","title":"Litecoin"},{"location":"data/#tezos","text":"WITH double_entry_book as ( SELECT IF(kind = 'contract', contract, delegate) AS address, change AS value FROM `public-data-finance.crypto_tezos.balance_updates` WHERE (status IS NULL OR status = 'applied') AND (timestamp < \"{{timestamp}}\") UNION ALL SELECT address, balance_change FROM `public-data-finance.crypto_tezos.migrations` WHERE timestamp < \"{{timestamp}}\" ) SELECT address, SUM(value) AS balance FROM double_entry_book GROUP BY address HAVING balance > 0 ORDER BY balance DESC","title":"Tezos"},{"location":"data/#zcash","text":"WITH double_entry_book AS ( SELECT array_to_string(inputs.addresses, \",\") as address, inputs.type, -inputs.value as value FROM `bigquery-public-data.crypto_zcash.inputs` as inputs WHERE block_timestamp < \"{{timestamp}}\" UNION ALL SELECT array_to_string(outputs.addresses, \",\") as address, outputs.type, outputs.value as value FROM `bigquery-public-data.crypto_zcash.outputs` as outputs WHERE block_timestamp < \"{{timestamp}}\" ) SELECT address, type, sum(value) as balance FROM double_entry_book GROUP BY 1,2 HAVING balance > 0 ORDER BY balance DESC","title":"Zcash"},{"location":"data/#automating-the-data-collection-process","text":"Instead of executing each of these queries separately on the BigQuery console and saving the results manually, it is also possible to automate the process using a script and collect all relevant data in one go. Executing this script will run queries from this file . IMPORTANT: the script uses service account credentials for authentication, therefore before running it, you need to generate the relevant credentials from Google, as described here and save your key in the data_collections_scripts/ directory of the project under the name 'google-service-account-key-0.json'. Any additional keys should be named 'google-service-account-key-1.json', 'google-service-account-key-2.json', and so on. There is a sample file that you can consult, which shows what your credentials are supposed to look like (but note that this is for informational purposes only, this file is not used in the code). Once you have set up the credentials, you can just run the following command from the root directory to retrieve data for all supported blockchains: python -m data_collection_scripts.big_query_balance_data There are also three command line arguments that can be used to customize the data collection process: ledgers accepts any number of the supported ledgers (case-insensitive). For example, adding --ledgers bitcoin results in collecting data only for Bitcoin, while --ledgers Bitcoin Ethereum would collect data for Bitcoin and Ethereum. If the ledgers argument is omitted, then the default value is used, which is taken from the configuration file and typically corresponds to all supported blockchains. snapshot_dates accepts any number of dates formatted as YYYY-MM-DD, YYYY-MM, or YYYY. Then, data is collected for the specified date(s). Again, if this argument is omitted, the default value is taken from the configuration file . --force-query forces the collection of all raw data files, even if some or all of the files already exist. By default, this flag is set to False and the script only fetches data for some blockchain if the corresponding file does not already exist.","title":"Automating the data collection process"},{"location":"metrics/","text":"Metrics The metrics that have been implemented so far are the following: Nakamoto coefficient : The Nakamoto coefficient represents the minimum number of entities that collectively control more than 50% of all tokens in circulation at a given point in time. The output of the metric is an integer. Gini coefficient : The Gini coefficient represents the degree of inequality in token ownership. The output of the metric is a decimal number in [0,1]. Values close to 0 indicate equality (all entities in the system control the same amount of assets) and values close to 1 indicate inequality (one entity holds most or all tokens). Entropy : Shannon entropy represents the expected amount of information in the distribution of tokens across entities. The output of the metric is a real number. Typically, a higher value of entropy indicates higher decentralization (lower predictability). HHI : The Herfindahl-Hirschman Index (HHI) is a measure of market concentration. It is defined as the sum of the squares of the market shares (as whole numbers, e.g. 40 for 40%) of the entities in the system. The output of the metric is a real number in (0, 10000]. Values close to 0 indicate low concentration (many entities hold a similar number of tokens) and values close to 10000 indicate high concentration (one entity controls most or all tokens). The U.S. Department of Justice has set the following thresholds for interpreting HHI values (in traditional markets): (0, 1500): Competitive market [1500, 2500]: Moderately concentrated market (2500, 10000]: Highly concentrated market Theil index : The Theil index is another measure of entropy which is intended to capture the lack of diversity, or the redundancy, in a population. In practice, it is calculated as the maximum possible entropy minus the observed entropy. The output is a real number. Values close to 0 indicate equality and values towards infinity indicate inequality. Therefore, a high Theil Index suggests a population that is highly centralized. Max power ratio : The max power ratio represents the share of tokens that are owned by the most \"powerful\" entity, i.e. the wealthiest entity. The output of the metric is a decimal number in [0,1]. Tau-decentralization index : The tau-decentralization index is a generalization of the Nakamoto coefficient. It is defined as the minimum number of entities that collectively control more than a given threshold of the total tokens in circulation. The threshold parameter is a decimal in [0, 1] (0.66 by default) and the output of the metric is an integer.","title":"Metrics"},{"location":"metrics/#metrics","text":"The metrics that have been implemented so far are the following: Nakamoto coefficient : The Nakamoto coefficient represents the minimum number of entities that collectively control more than 50% of all tokens in circulation at a given point in time. The output of the metric is an integer. Gini coefficient : The Gini coefficient represents the degree of inequality in token ownership. The output of the metric is a decimal number in [0,1]. Values close to 0 indicate equality (all entities in the system control the same amount of assets) and values close to 1 indicate inequality (one entity holds most or all tokens). Entropy : Shannon entropy represents the expected amount of information in the distribution of tokens across entities. The output of the metric is a real number. Typically, a higher value of entropy indicates higher decentralization (lower predictability). HHI : The Herfindahl-Hirschman Index (HHI) is a measure of market concentration. It is defined as the sum of the squares of the market shares (as whole numbers, e.g. 40 for 40%) of the entities in the system. The output of the metric is a real number in (0, 10000]. Values close to 0 indicate low concentration (many entities hold a similar number of tokens) and values close to 10000 indicate high concentration (one entity controls most or all tokens). The U.S. Department of Justice has set the following thresholds for interpreting HHI values (in traditional markets): (0, 1500): Competitive market [1500, 2500]: Moderately concentrated market (2500, 10000]: Highly concentrated market Theil index : The Theil index is another measure of entropy which is intended to capture the lack of diversity, or the redundancy, in a population. In practice, it is calculated as the maximum possible entropy minus the observed entropy. The output is a real number. Values close to 0 indicate equality and values towards infinity indicate inequality. Therefore, a high Theil Index suggests a population that is highly centralized. Max power ratio : The max power ratio represents the share of tokens that are owned by the most \"powerful\" entity, i.e. the wealthiest entity. The output of the metric is a decimal number in [0,1]. Tau-decentralization index : The tau-decentralization index is a generalization of the Nakamoto coefficient. It is defined as the minimum number of entities that collectively control more than a given threshold of the total tokens in circulation. The threshold parameter is a decimal in [0, 1] (0.66 by default) and the output of the metric is an integer.","title":"Metrics"},{"location":"setup/","text":"Setup Installation To install the tokenomics decentralization analysis tool, simply clone this GitHub repository: git clone https://github.com/Blockchain-Technology-Lab/tokenomics-decentralization.git The tool is written in Python 3, therefore a Python 3 interpreter is required in order to run it locally. The requirements file lists the dependencies of the project. Make sure you have all of them installed before running the scripts. To install all of them in one go, run the following command from the root directory of the project: python -m pip install -r requirements.txt Execution The tokenomics decentralization analysis tool is a CLI tool. To run the tool simply do: python run.py The execution is controlled and parameterized by the configuration file config.yml as follows. metrics defines the metrics that should be computed in the analysis. By default all supported metrics are included here (to add support for a new metric see the conributions page ). ledgers defines the ledgers that should be analyzed. By default, all supported ledgers are included here (to add support for a new ledger see the conributions page ). execution_flags defines various flags that control the data handling: force_map_addresses : the address helper data from the directory mapping_information is re-computed; you should set this flag to true if the data has been updated since the last execution for the given ledger force_map_balances : the balance data of the ledger's addresses is recomputed; you should set this flag to true if the data has been updated since the last execution for the given ledger force_analyze : the computation of a metric is recomputed; you should set this flag to true if any type of data has been updated since the last execution for the given ledger analyze_flags defines various analysis-related flags: clustering : a boolean that determines whether addresses will be clustered into entities (as defined in the mapping information). If set to False, no clustering takes place and the addresses are treated as distinct entities. top_limit_type : a string of two values ( absolute or percentage ) that enables applying a threshold on the addresses that will be considered top_limit_value : the value of the top limit that should be applied; if 0, then no limit is used (regardless of the value of top_limit_type ); if the type is absolute , then the top_limit_value should be an integer (e.g., if set to 100, then only the 100 wealthiest entities/addresses will be considered in the analysis); if the type is percentage the the top_limit_value should be an integer (e.g., if set to 0.50, then only the top 50% of wealthiest entities/addresses will be considered) exclude_contract_addresses : a boolean value that enables the exclusion of contract addresses from the analysis exclude_below_usd_cent : a boolean value that enables the exclusion of addresses, the balance of which at the analyzed point in time was less than $0.01 (based on the historical price information in the directory price_data ) snapshot_dates and granularity control the snapshots for which an analysis will be performed. granularity is a string that can be empty or one of day , week , month , year . If granularity is empty, then snapshot_dates define the exact time points for which an analysis will be conducted, in the form YYYY-MM-DD. Otherwise, if granularity is set, then the two farthest entries in snapshot_dates define the timeframe over which the analysis will be conducted, at the set granular rate. For example, if the farthest points are 2010 and 2023 and the granularity is set to month , then (the first day of) every month in the years 2010-2023 (inclusive) will be analyzed. input_directories and output_directories are both lists of directories that define the source of data. input_directories defines the directories that contain raw address balance information, as obtained from BigQuery or a full node (for more information about this see the data collection page ). output_directories defines the directories to store the databases which contain the mapping information and analyzed data. The first entry in the output directories is also used to store the output files of the analysis and the plots. Finally, plot_parameters contains various parameters that control the type and data that will be produced as plots. ...","title":"How to use"},{"location":"setup/#setup","text":"","title":"Setup"},{"location":"setup/#installation","text":"To install the tokenomics decentralization analysis tool, simply clone this GitHub repository: git clone https://github.com/Blockchain-Technology-Lab/tokenomics-decentralization.git The tool is written in Python 3, therefore a Python 3 interpreter is required in order to run it locally. The requirements file lists the dependencies of the project. Make sure you have all of them installed before running the scripts. To install all of them in one go, run the following command from the root directory of the project: python -m pip install -r requirements.txt","title":"Installation"},{"location":"setup/#execution","text":"The tokenomics decentralization analysis tool is a CLI tool. To run the tool simply do: python run.py The execution is controlled and parameterized by the configuration file config.yml as follows. metrics defines the metrics that should be computed in the analysis. By default all supported metrics are included here (to add support for a new metric see the conributions page ). ledgers defines the ledgers that should be analyzed. By default, all supported ledgers are included here (to add support for a new ledger see the conributions page ). execution_flags defines various flags that control the data handling: force_map_addresses : the address helper data from the directory mapping_information is re-computed; you should set this flag to true if the data has been updated since the last execution for the given ledger force_map_balances : the balance data of the ledger's addresses is recomputed; you should set this flag to true if the data has been updated since the last execution for the given ledger force_analyze : the computation of a metric is recomputed; you should set this flag to true if any type of data has been updated since the last execution for the given ledger analyze_flags defines various analysis-related flags: clustering : a boolean that determines whether addresses will be clustered into entities (as defined in the mapping information). If set to False, no clustering takes place and the addresses are treated as distinct entities. top_limit_type : a string of two values ( absolute or percentage ) that enables applying a threshold on the addresses that will be considered top_limit_value : the value of the top limit that should be applied; if 0, then no limit is used (regardless of the value of top_limit_type ); if the type is absolute , then the top_limit_value should be an integer (e.g., if set to 100, then only the 100 wealthiest entities/addresses will be considered in the analysis); if the type is percentage the the top_limit_value should be an integer (e.g., if set to 0.50, then only the top 50% of wealthiest entities/addresses will be considered) exclude_contract_addresses : a boolean value that enables the exclusion of contract addresses from the analysis exclude_below_usd_cent : a boolean value that enables the exclusion of addresses, the balance of which at the analyzed point in time was less than $0.01 (based on the historical price information in the directory price_data ) snapshot_dates and granularity control the snapshots for which an analysis will be performed. granularity is a string that can be empty or one of day , week , month , year . If granularity is empty, then snapshot_dates define the exact time points for which an analysis will be conducted, in the form YYYY-MM-DD. Otherwise, if granularity is set, then the two farthest entries in snapshot_dates define the timeframe over which the analysis will be conducted, at the set granular rate. For example, if the farthest points are 2010 and 2023 and the granularity is set to month , then (the first day of) every month in the years 2010-2023 (inclusive) will be analyzed. input_directories and output_directories are both lists of directories that define the source of data. input_directories defines the directories that contain raw address balance information, as obtained from BigQuery or a full node (for more information about this see the data collection page ). output_directories defines the directories to store the databases which contain the mapping information and analyzed data. The first entry in the output directories is also used to store the output files of the analysis and the plots. Finally, plot_parameters contains various parameters that control the type and data that will be produced as plots. ...","title":"Execution"}]}